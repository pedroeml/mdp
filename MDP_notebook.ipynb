{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDPs in pymdptoolbox - Class Assignment\n",
    "\n",
    "In this practical exercise, we will look at how MDP planning is implemented in a mathematical toolkit, and track the calculation of the rewards for each state via Value Iteration. The following code sets up an MDP environment (the basic case shown in class, shown in the Figure below) and computes the policy for the given MDP using the Value Iteration algorithm.\n",
    "\n",
    "<img align=\"center\" src=\"mdp_simple.png\"/>\n",
    "\n",
    "Then we provide a set of questions for you to implement and answer. This assignment is not graded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'pymdptoolbox/src')\n",
    "import mdptoolbox.example\n",
    "\n",
    "import numpy as _np\n",
    "from gen_scenario import *\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration   Variation\n",
      "         1  200.000000\n",
      "         2   41.260000\n",
      "         3   16.483000\n",
      "         4    6.465697\n",
      "         5    1.551601\n",
      "         6    0.295707\n",
      "         7    0.060178\n",
      "         8    0.015785\n",
      "         9    0.005540\n",
      "        10    0.002253\n",
      "        11    0.000981\n",
      "Iterating stopped, epsilon-optimal policy found.\n",
      "[['E' 'E' 'E' 'T']\n",
      " ['N' 'O' 'N' 'T']\n",
      " ['N' 'E' 'N' 'S']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"font-size:300%;border: thick solid;\"><tr><td>&rarr;</td><td>&rarr;</td><td>&rarr;</td><td>&#x25CE;</td></tr><tr><td>&uarr;</td><td>&#x25FE;</td><td>&uarr;</td><td>&#x25CE;</td></tr><tr><td>&uarr;</td><td>&rarr;</td><td>&uarr;</td><td>&darr;</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The line below is to be used if you have pymdptoolbox installed with setuptools\n",
    "# import mdptoolbox.example\n",
    "# Whereas the line below obviate the need to install that\n",
    "\n",
    "\"\"\"\n",
    "(Y,X)\n",
    "| 00 01 02 ... 0X-1       'N' = North\n",
    "| 10  .         .         'S' = South\n",
    "| 20    .       .         'W' = West\n",
    "| .       .     .         'E' = East\n",
    "| .         .   .         'T' = Terminal\n",
    "| .           . .         'O' = Obstacle\n",
    "| Y-1,0 . . .   Y-1X-1\n",
    "\"\"\" \n",
    "\n",
    "shape = [3,4]\n",
    "rewards = [[0,3,100],[1,3,-100]]\n",
    "obstacles = [[1,1]]\n",
    "terminals = [[0,3],[1,3]]\n",
    "P, RSS, R = mdp_grid(shape=shape, terminals=terminals, r=-3, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIterationGS(P, R, discount=0.5, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.verbose = True\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n",
    "display_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'lineplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-65806f6e4e97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata_frame2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'difference'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'discount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'iterations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'difference'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discount'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_frame2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-65806f6e4e97>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(P, R, discounts, epsilon, max_iter)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0miterations\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata_frame2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'difference'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'discount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'iterations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'difference'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discount'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_frame2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'lineplot'"
     ]
    }
   ],
   "source": [
    "def plot(P, R, discounts=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95], epsilon=0.001, max_iter=1000):\n",
    "    data_list = []\n",
    "    for d in discounts:\n",
    "        vis = mdptoolbox.mdp.ValueIteration(P, R, d, epsilon, max_iter, skip_check=True)\n",
    "        vis.run()\n",
    "        iterations = 1\n",
    "        for value in vis.iterations_list:\n",
    "            data_list.append([value, 'gamma: ' + str(d), iterations])\n",
    "            iterations +=1\n",
    "    data_frame2 = pd.DataFrame(data_list, columns=['difference', 'discount', 'iterations'])\n",
    "    ax = sns.lineplot(x = 'iterations', y = 'difference', hue='discount',  data=data_frame2)\n",
    "plot(P,R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following commands to check the difference of value-function values during each teration and the values of each value function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200.0, 41.260000000000005, 16.483000000000004, 6.4656972263671886, 1.5516013086428719, 0.29570678878162315, 0.060177953646783244, 0.015784916353677847, 0.0055399796787702904, 0.0022526739209762781, 0.00098114810443483691]\n",
      "[   2.69494788   14.16745349   39.37677054  100.           -2.13558073\n",
      "   -3.            8.15864023 -100.           -4.23674139   -3.41357286\n",
      "   -0.18068356   -5.47018326]\n"
     ]
    }
   ],
   "source": [
    "print(vi.iterations_list)\n",
    "print(vi.v_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you go for the questionnaire, take your time to open the source code of the MDP toolkit we use, specifically, look into these files:\n",
    "1. [gen_scenario.py](gen_scenario.py) - contains the conversion code to make the simple coordinate commands above (e.g. ```shape = [3,4]```) into the matrices actually used by the MDP solver\n",
    "2. [mdp.py](pymdptoolbox/src/mdptoolbox/mdp.py) - contains most of the logic for an MDP, including the *Bellman Equation* as follows:\n",
    "\n",
    "$$V(s) = \\left[ \\max_{a} \\gamma \\sum_{s'}P(s'|s,a)*V(s') \\right]+ R(s)$$\n",
    "\n",
    "See if you can identify how this equation is implemented in the ```MDP._bellmanOperator``` with the [mdp.py](pymdptoolbox/src/mdptoolbox/mdp.py) file. Note how this implementation uses matrix multiplication to achieve the summation step described in the equation. Once you believe you understand that, go ahead and respond the questionnaire. \n",
    "\n",
    "### Questionnaire\n",
    "1. Study the code of the cell above and answer the following questions.\n",
    "\t1. What is the policy generated if we change the discount factor of the grid domain to ```0.1```?\n",
    "\t2. Use the following line ```vi.verbose = True``` before ```vi.run()```:   \n",
    "\tWhat is the variation for each of the first three iterations with the discount factor of ```0.9``` and how many iterations does the algorithm take to converge?\n",
    "\t3. How does changes to the discount factor affect the variation of the state values over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.A\n",
    "# [['E' 'E' 'E' 'T']\n",
    "#  ['N' 'O' 'W' 'T']\n",
    "#  ['N' 'E' 'N' 'S']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.B\n",
    "# Iteration   Variation\n",
    "#          1  200.000000\n",
    "#          2   74.354400\n",
    "#          3   53.412696\n",
    "#             ...\n",
    "#         16    0.000057\n",
    "# Iterating stopped, epsilon-optimal policy found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1.C\n",
    "# It not only affect the pecentage of discount over the variation at each iteration, it also\n",
    "# affects the number of iterations to reach optimal result because the more it's closer to 1\n",
    "# the more it ignores future rewards over each iteration.\n",
    "# For discount = 0.1 the optimal result is at the 5th iteration.\n",
    "# Iteration   Variation\n",
    "#          1  200.000000\n",
    "#          2    8.242400\n",
    "#          3    0.659224\n",
    "#          4    0.052638\n",
    "#          5    0.002211\n",
    "# If it increases to 0.5 the optimal result is only reached ath the 11th.\n",
    "# Iteration   Variation\n",
    "#         1  200.000000\n",
    "#         2   41.260000\n",
    "#         3   16.483000\n",
    "#          ...\n",
    "#        10    0.002253\n",
    "#        11    0.000981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The scenario below has an interesting structure whereby the positive rewarding terminal state is partially surrounded by negatively-rewarding states. Program this scenario in pymdptoolbox and compute the optimal policy with a discount factor of 0.99.\n",
    "\n",
    "<img align=\"center\" src=\"mdp-odd.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration   Variation\n",
      "         1  200.000000\n",
      "         2   53.426516\n",
      "         3   41.785472\n",
      "         4   24.594519\n",
      "         5   20.056584\n",
      "         6   17.358982\n",
      "         7   14.044100\n",
      "         8   13.179184\n",
      "         9   10.020322\n",
      "        10    6.183187\n",
      "        11    5.195060\n",
      "        12    3.163524\n",
      "        13    1.621558\n",
      "        14    0.764909\n",
      "        15    0.341000\n",
      "        16    0.145604\n",
      "        17    0.060068\n",
      "        18    0.024092\n",
      "        19    0.009439\n",
      "        20    0.003626\n",
      "        21    0.001370\n",
      "        22    0.000510\n",
      "        23    0.000187\n",
      "        24    0.000068\n",
      "        25    0.000025\n",
      "        26    0.000009\n",
      "Iterating stopped, epsilon-optimal policy found.\n",
      "[['E' 'E' 'E' 'S' 'W']\n",
      " ['N' 'O' 'T' 'S' 'O']\n",
      " ['S' 'T' 'T' 'W' 'W']\n",
      " ['S' 'O' 'T' 'O' 'N']\n",
      " ['E' 'E' 'E' 'E' 'N']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"font-size:300%;border: thick solid;\"><tr><td>&rarr;</td><td>&rarr;</td><td>&rarr;</td><td>&darr;</td><td>&larr;</td></tr><tr><td>&uarr;</td><td>&#x25FE;</td><td>&#x25CE;</td><td>&darr;</td><td>&#x25FE;</td></tr><tr><td>&darr;</td><td>&#x25CE;</td><td>&#x25CE;</td><td>&larr;</td><td>&larr;</td></tr><tr><td>&darr;</td><td>&#x25FE;</td><td>&#x25CE;</td><td>&#x25FE;</td><td>&uarr;</td></tr><tr><td>&rarr;</td><td>&rarr;</td><td>&rarr;</td><td>&rarr;</td><td>&uarr;</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2\n",
    "shape = [5,5]\n",
    "rewards = [[2,1,-100],[1,2,-100],[2,2,100],[3,2,-100]]\n",
    "obstacles = [[1,1],[1,4],[3,1],[3,3]]\n",
    "terminals = [[2,1],[1,2],[2,2],[3,2]]\n",
    "P, RSS, R = mdp_grid(shape=shape, terminals=terminals, r=-3, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIterationGS(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "vi.verbose = True\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n",
    "display_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define two new 5 by 5 scenarios with multiple obstacles and an interesting geometry following the guidelines below. Calculate the policy with discount factor 0.99, and then try to explain intuitively the reason for the resulting policies, given the initial parameters. These two scenarios must have the following characteristics:\n",
    "\t1. A scenario with one (or more) terminal states with positive rewards and at least one other state with the same amount of, but negative reward and no terminal states with negative rewards.\n",
    "\t2. A scenario with one terminal state with a negative reward and at least one non-terminal state with a positive reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E' 'E' 'S' 'E' 'S']\n",
      " ['N' 'N' 'S' 'O' 'S']\n",
      " ['N' 'E' 'E' 'T' 'W']\n",
      " ['E' 'E' 'N' 'O' 'O']\n",
      " ['N' 'E' 'E' 'E' 'T']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"font-size:300%;border: thick solid;\"><tr><td>&rarr;</td><td>&rarr;</td><td>&darr;</td><td>&rarr;</td><td>&darr;</td></tr><tr><td>&uarr;</td><td>&uarr;</td><td>&darr;</td><td>&#x25FE;</td><td>&darr;</td></tr><tr><td>&uarr;</td><td>&rarr;</td><td>&rarr;</td><td>&#x25CE;</td><td>&larr;</td></tr><tr><td>&rarr;</td><td>&rarr;</td><td>&uarr;</td><td>&#x25FE;</td><td>&#x25FE;</td></tr><tr><td>&uarr;</td><td>&rarr;</td><td>&rarr;</td><td>&rarr;</td><td>&#x25CE;</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### 3.A\n",
    "shape = [5, 5]\n",
    "rewards = [[2, 3, 100], [2, 1, -100], [4, 1, -100], [4, 4, 100]]\n",
    "obstacles = [[1, 3], [3, 3], [3, 4]]\n",
    "terminals = [[2, 3], [4, 4]]\n",
    "P, RSS, R = mdp_grid(shape=shape, terminals=terminals, r=-3, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIterationGS(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "# vi.verbose = True\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n",
    "display_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['E' 'E' 'S' 'W' 'S']\n",
      " ['E' 'E' 'S' 'O' 'S']\n",
      " ['O' 'O' 'S' 'W' 'W']\n",
      " ['E' 'N' 'S' 'O' 'O']\n",
      " ['W' 'T' 'E' 'E' 'E']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"font-size:300%;border: thick solid;\"><tr><td>&rarr;</td><td>&rarr;</td><td>&darr;</td><td>&larr;</td><td>&darr;</td></tr><tr><td>&rarr;</td><td>&rarr;</td><td>&darr;</td><td>&#x25FE;</td><td>&darr;</td></tr><tr><td>&#x25FE;</td><td>&#x25FE;</td><td>&darr;</td><td>&larr;</td><td>&larr;</td></tr><tr><td>&rarr;</td><td>&uarr;</td><td>&darr;</td><td>&#x25FE;</td><td>&#x25FE;</td></tr><tr><td>&larr;</td><td>&#x25CE;</td><td>&rarr;</td><td>&rarr;</td><td>&rarr;</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3.B\n",
    "shape = [5, 5]\n",
    "rewards = [[2, 3, 100], [4, 1, -100], [4, 4, 100]]\n",
    "obstacles = [[2, 0], [2, 1], [1, 3], [3, 3], [3, 4]]\n",
    "terminals = [[4, 1]]\n",
    "P, RSS, R = mdp_grid(shape=shape, terminals=terminals, r=-3, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIterationGS(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "#vi.verbose = True\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n",
    "display_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
